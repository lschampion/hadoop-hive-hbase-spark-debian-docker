################################-- Start Packaging --######################################
FROM debian:stable as env_package
VOLUME [ "/sys/fs/cgroup" ]

# https://github.com/hadolint/hadolint/wiki/DL4006
SHELL ["/bin/bash", "-o", "pipefail", "-c"]

# RUN echo deb http://deb.debian.org/debian buster main > /etc/apt/sources.list \
#  && echo deb http://security.debian.org/debian-security buster/updates main  >> /etc/apt/sources.list \
#  && echo deb http://deb.debian.org/debian buster-updates main  >> /etc/apt/sources.list 

RUN apt-get update --fix-missing
RUN apt-get install -y curl wget \
 && apt-get install -y zip \
 && apt-get install -y vim \
 && apt-get install -y net-tools iputils-ping netcat coreutils \
 && apt-get install -y tcl tk expect 
RUN apt-get install -y locales \
 && localedef -i en_US -c -f UTF-8 -A /usr/share/locale/locale.alias en_US.UTF8 

# JAVA install
RUN apt install -y apt-transport-https ca-certificates  dirmngr gnupg software-properties-common \
 && wget -qO - https://adoptopenjdk.jfrog.io/adoptopenjdk/api/gpg/key/public | apt-key add - \
 && add-apt-repository --yes https://adoptopenjdk.jfrog.io/adoptopenjdk/deb/ \
 && apt-get update \
 && apt install -y adoptopenjdk-8-hotspot  \
 && java -version \
 && export JAVA_DIRECTORY=`ls -l /usr/lib/jvm | grep ^d | grep jdk | awk '{print $9}'` \
 && ln -s /usr/lib/jvm/$JAVA_DIRECTORY /usr/lib/jvm/java_home \
 && echo JAVA_HOME=/usr/lib/jvm/java_home >> /etc/profile \
 && echo 'JRE_HOME=$JAVA_HOME/jre' >> /etc/profile \
 && echo 'CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib' >> /etc/profile \
 && source /etc/profile 


# ssh install 设置默认root密码
ENV ROOT_PWD=123456
RUN apt-get install -y openssh-server \
 && cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime  \
 && echo "root:${ROOT_PWD}" | chpasswd \
 && sed -i "s/^#PasswordAuthentication.*/PasswordAuthentication yes"/g /etc/ssh/sshd_config \
 && sed -i "s/^#PermitRootLogin.*/PermitRootLogin yes"/g /etc/ssh/sshd_config \
 && sed -i "s/^#Port.*/Port 22"/g /etc/ssh/sshd_config \
 && service ssh start \
 && service ssh status \
 && update-rc.d ssh enable \
 && mkdir -p /root/.ssh && chmod 700 /root/.ssh/ \
 && ssh-keygen -t rsa -N '' -f /root/.ssh/id_rsa \
 && echo "alias ll='ls -l --color=auto'" >> ~/.bashrc && source ~/.bashrc

# python install
# PySpark - comment out if you don't want it in order to save image space
RUN apt install -y curl build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libsqlite3-dev libreadline-dev libffi-dev libbz2-dev \
 && wget https://www.python.org/ftp/python/3.7.5/Python-3.7.5.tgz \
 && tar -xf Python-3.7.5.tgz \
 && cd Python-3.7.5 \
 && ./configure && make && make install \
 && ln -s /usr/local/bin/python3 /usr/bin/python \
 && ln -s /usr/local/bin/pip3 /usr/bin/pip \
 && which python3 \
 && cd ../ && rm -rf Python-3.7.5 Python-3.7.5.tgz

ENV USR_PROGRAM_DIR=/usr/program
ENV USR_BIN_DIR="${USR_PROGRAM_DIR}/source_dir"
RUN mkdir -p "${USR_BIN_DIR}"

# Common settings
ENV JAVA_HOME /usr/lib/jvm/java_home
ENV PATH="${PATH}:${JAVA_HOME}/bin"
# http://blog.stuart.axelbrooke.com/python-3-on-spark-return-of-the-pythonhashseed
ENV PYTHONHASHSEED 0
ENV PYTHONIOENCODING UTF-8
ENV PIP_DISABLE_PIP_VERSION_CHECK 1

# Hadoop
ENV HADOOP_VERSION=3.2.3
ENV HADOOP_HOME /usr/program/hadoop
ENV HADOOP_NNAMENADE_HOSTNAME=master
ENV HADOOP_PACKAGE="hadoop-${HADOOP_VERSION}.tar.gz"
ENV PATH="${PATH}:${HADOOP_HOME}/bin"
ENV PATH="${PATH}:${HADOOP_HOME}/sbin"
ENV HDFS_NAMENODE_USER="root"
ENV HDFS_DATANODE_USER="root"
ENV HDFS_SECONDARYNAMENODE_USER="root"
ENV YARN_RESOURCEMANAGER_USER="root"
ENV YARN_NODEMANAGER_USER="root"
ENV LD_LIBRARY_PATH="${HADOOP_HOME}/lib/native:${LD_LIBRARY_PATH}"
ENV HADOOP_CONF_DIR="${HADOOP_HOME}/etc/hadoop"
ENV HADOOP_LOG_DIR="${HADOOP_HOME}/logs"
# For S3 to work. Without this line you'll get "Class org.apache.hadoop.fs.s3a.S3AFileSystem not found" exception when accessing S3 from Hadoop
ENV HADOOP_CLASSPATH="${HADOOP_HOME}/share/hadoop/tools/lib/*"

# Hive
ENV HIVE_VERSION=3.1.2
ENV HIVE_HOME=/usr/program/hive
ENV HIVE_CONF_DIR="${HIVE_HOME}/conf"
ENV HIVE_LOG_DIR="${HIVE_HOME}/logs"
ENV HIVE_PACKAGE="apache-hive-${HIVE_VERSION}-bin.tar.gz"
ENV PATH="${PATH}:${HIVE_HOME}/bin"
ENV HADOOP_CLASSPATH="${HADOOP_CLASSPATH}:${HIVE_HOME}/lib/*"
# Hive Hudi support
ENV HIVE_AUX_JARS_PATH=/usr/program/hive/lib/hudi-hadoop-mr-bundle-0.10.0.jar,/usr/program/hive/lib/hudi-hive-sync-bundle-0.10.0.jar

# HBase
ENV HBASE_VERSION=2.3.6
ENV HBASE_HOME=/usr/program/hbase
ENV HBASE_CONF_DIR="${HBASE_HOME}/conf/"
ENV HBASE_PACKAGE="hbase-${HBASE_VERSION}-bin.tar.gz"
ENV PATH="${PATH}:${HBASE_HOME}/bin"
ENV HBASE_LOG_DIR="${HBASE_HOME}/logs"

# Spark
ENV SPARK_VERSION=3.0.0
ENV SPARK_HADOOP_VERSION=3.2
ENV SPARK_HOME=/usr/program/spark
ENV SPARK_PACKAGE="spark-${SPARK_VERSION}-bin-hadoop${SPARK_HADOOP_VERSION}.tgz"
ENV PATH="${PATH}:${SPARK_HOME}/bin"
ENV SPARK_CONF_DIR="${SPARK_HOME}/conf"
ENV SPARK_LOG_DIR="${SPARK_HOME}/logs"
ENV SPARK_DIST_CLASSPATH="${HADOOP_CONF_DIR}:${HADOOP_HOME}/share/hadoop/tools/lib/*:${HADOOP_HOME}/share/hadoop/common/lib/*:${HADOOP_HOME}/share/hadoop/common/*:${HADOOP_HOME}/share/hadoop/hdfs:${HADOOP_HOME}/share/hadoop/hdfs/lib/*:${HADOOP_HOME}/share/hadoop/hdfs/*:${HADOOP_HOME}/share/hadoop/mapreduce/lib/*:${HADOOP_HOME}/share/hadoop/mapreduce/*:${HADOOP_HOME}/share/hadoop/yarn:${HADOOP_HOME}/share/hadoop/yarn/lib/*:${HADOOP_HOME}/share/hadoop/yarn/*"

# FLINK
ENV FLINK_VERSION=1.13.6
ENV FLINK_SCALA_VERSION=2.12
ENV FLINK_HOME=/usr/program/flink
ENV FLINK_PACKAGE="flink-${FLINK_VERSION}-bin-scala_${FLINK_SCALA_VERSION}.tgz"
ENV PATH="${PATH}:${FLINK_HOME}/bin"
ENV FLINK_CONF_DIR="${FLINK_HOME}/conf"
ENV FLINK_LOG_DIR="${FLINK_HOME}/logs"

# Sqoop
ENV SQOOP_VERSION=1.4.7
ENV HADOOP_SQOOP_VERSION=2.6.0
ENV SQOOP_HOME=/usr/program/sqoop
ENV SQOOP_PACKAGE="sqoop-${SQOOP_VERSION}.bin__hadoop-${HADOOP_SQOOP_VERSION}.tar.gz"
ENV PATH="${PATH}:${SQOOP_HOME}/bin"
ENV HADOOP_COMMON_HOME="${HADOOP_HOME}"
ENV HADOOP_MAPRED_HOME="${HADOOP_HOME}"
ENV SQOOP_CONF_DIR="${SQOOP_HOME}/conf"
ENV SQOOP_LOG_DIR="${SQOOP_HOME}/logs"

# Zookeeper
ENV ZK_VERSION=3.6.3
ENV ZK_HOME=/usr/program/zookeeper
ENV ZK_CONF_DIR=${ZK_HOME}/conf
ENV ZK_PACKAGE="apache-zookeeper-${ZK_VERSION}-bin.tar.gz"

# Tez
ENV TEZ_VERSION=0.9.2
ENV TEZ_HOME=/usr/program/tez
ENV TEZ_PACKAGE="apache-tez-${TEZ_VERSION}-bin.tar.gz"
ENV TEZ_CONF_DIR=${HADOOP_CONF_DIR}
ENV TEZ_JARS=${TEZ_HOME}/*:${TEZ_HOME}/lib/*
ENV HADOOP_CLASSPATH=${TEZ_CONF_DIR}:${TEZ_JARS}:${HADOOP_CLASSPATH}

# Hudi
ENV HUDI_VERSION=0.10.0

# Trino(PrestoSQL)
ENV TRINO_VERSION=378
ENV TRINO_HOME=/usr/program/trino-server
ENV TRINO_CONF_DIR=${TRINO_HOME}/etc
ENV ZULU_JDK11_HOME=/usr/program/zulu-jdk11

# 220426: 0.8.0 is not a release version
ENV LIVY_VERSION=0.8.0-incubating
ENV LIVY_HOME=/usr/program/livy
ENV LIVY_CONF_DIR="${LIVY_HOME}/conf"
ENV LIVY_LOG_DIR="${LIVY_HOME}/logs"
ENV LIVY_PACKAGE="apache-livy-${LIVY_VERSION}-bin.zip"


# Multi tail for logging 多重日志合并服务
COPY scripts/ /scripts
RUN gcc /scripts/watchdir.c -o /scripts/watchdir && chmod +x /scripts/*
################################-- end Packaging --######################################

